<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <title>    Andrew Ng's ML Week 06, 11
</title>
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta content="This is the homepage of Zeyuan Hu" name="description">
        <meta content="Zeyuan Hu, Zeyuan, zeyuan hu, zeyuan ibm, IBM, Zeyuan IBM, UW Madison, University of Wisconsin Madison, zeyuan wisc, zeyuan IBM, zeyuan federation" name="keywords">
        <meta content="Zeyuan Hu" name="author">
        <link href='https://fonts.googleapis.com/css?family=Gentium+Book+Basic|Merriweather:400,300' rel='stylesheet' type='text/css'>
        <link rel="stylesheet" href="../../../../../theme/css/cid.css">
        <!-- add font-awesome -->
        <script defer src="../../../../../theme/fa-5/js/all.js"></script>
        <link rel="stylesheet" href="../../../../../theme/academicons/css/academicons.css"/>
        <link href="https://zhu45.org/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Zeyuan Hu's page Atom Feed" />
        <link href="https://zhu45.org/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="Zeyuan Hu's page RSS Feed" />
        <link href="../../../../../theme/images/favicon.ico" rel="icon">
    </head>
    <body>
        <!--[if lt IE 7]>
            <p class="chromeframe">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a> to improve your experience.</p>
        <![endif]-->
            <div class="container">
<header class="blog-header">
        <h1 id="site-title"><a href="../../../../.." style="color: black; text-decoration: none">Zeyuan Hu's page</a></h1>
    <p></p>
    <nav>
            <a href="../../../../../about-me.html" style="padding: 10px">ABOUT</a>
            <a href="../../../../../archives/index.html" style="padding: 10px">ARCHIVES</a>
            <a href="../../../../../projects.html" style="padding: 10px">PROJECTS</a>
    </nav>
</header>
    <div class="post">
        <header>
          <div class="post-title">Andrew Ng's ML Week 06, 11</h1></div>
          <div class="post-date"><time datetime="2017-07-21T12:51:00+08:00">Jul 21, 2017</time></div>
        </header>
        
        <article>
            <p>I actually <a href="https://www.coursera.org/account/accomplishments/verify/58VFP5LF4UXV">finished the course</a> 
on June 25th. In this page, I'll
summarize various advices and tips given by Prof. Andrew Ng on how to build
a effective machine learning system.</p>
<p>To be honest, I used to think this part of material may be not worth a post 
but as I dig deeper into the course I find out that this part is invaluable 
because it answers some commonly-seen questions when implementing a machine learning
system, which can be a huge time-saver. So, I think I need a post to record
those advices systematically. </p>
<div class="toc">
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#diagnostics">Diagnostics</a></li>
<li><a href="#overfitting-vs-underfitting">Overfitting vs. Underfitting</a><ul>
<li><a href="#model-selection-algorithm">Model selection algorithm</a></li>
<li><a href="#diagnosing-bias-vs-variance-which-is-which">Diagnosing bias vs. variance: which is which?</a></li>
<li><a href="#regularization-how-to-choose-lambda">Regularization: how to choose \(\lambda\)?</a></li>
<li><a href="#learning-curves">Learning curves</a><ul>
<li><a href="#experience-underfitting">Experience underfitting</a></li>
<li><a href="#experience-overfitting">Experience overfitting</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#what-to-try-next">What to try next?</a></li>
<li><a href="#neural-network-and-overfitting">Neural Network and overfitting</a></li>
<li><a href="#error-analysis">Error analysis</a></li>
<li><a href="#ceiling-analysis">Ceiling analysis</a></li>
<li><a href="#other-issues">Other issues</a><ul>
<li><a href="#error-metrics-for-skewed-classes-precision-recall">Error metrics for skewed classes: Precision &amp; Recall</a><ul>
<li><a href="#skew-classes">Skew classes</a></li>
<li><a href="#precision-recall">Precision &amp; Recall</a></li>
<li><a href="#f_1-score">\(F_1\) score</a></li>
</ul>
</li>
<li><a href="#data-for-machine-learning">Data for machine learning</a></li>
</ul>
</li>
</ul>
</div>
<h2 id="preface">Preface</h2>
<p>One important question we may ask after implementing our machine learning algorithm
is that: how good is our learning algorithm? In addition, for instance, after
we have implemented regularized linear regression to predict housing prices 
and when we test our hypothesis on a new set of houses, we find that it makes
unacceptably large errors in the predictions, what we should try next? This post
aims to answer those questions.</p>
<p>In this post, I will first take a look at the diagnostic to evaluate learning algorithm.
Then, I will define overfitting (high bias) and underfitting (high variance)
concepts and the concrete techniques to identify which is which. Afterwards, I will
talk about several ways to handle the problem and highlight some key points. Lastly,
we will take a look at some special cases when data is skewed or large.</p>
<h2 id="diagnostics">Diagnostics</h2>
<p><strong>Diagnostics</strong> is a test that you can run to gain insight what is or isn't working
with a learning algorithm, and gain guidance as to how best to improve its performance.
We use <strong>test set error</strong> as our basic metrics to evaulate our learning algorithm (hypothesis).</p>
<p>We first shuffle our whole data set to eliminate the potential impact of data record
ordering. Then, we randomly choose <span class="math">\(70\%\)</span> of data set as our training set and the rest
<span class="math">\(30\%\)</span> as our test set. Mathematically, we denote training set: 
<span class="math">\((x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \dots, (x^{(m)}, y^{(m)})\)</span> and we denote
test set as <span class="math">\((x_{\text{test}}^{(1)}, y_{\text{test}}^{(1)}), (x_{\text{test}}^{(1)}, y_{\text{test}}^{(1)}) \dots (x_{\text{test}}^{(m_\text{test})}, y_{\text{test}}^{(m_\text{test})})\)</span> with
<span class="math">\(m_\text{test} = \text{no. of test examples}\)</span>.</p>
<ul>
<li>
<p>For linear regression, our test set error is calculated by the following steps:</p>
<ol>
<li>Learn parameters <span class="math">\(\theta\)</span> from training data (i.e. minimizing training error <span class="math">\(J(\theta)\)</span>)</li>
<li>compute the test set error as follows:</li>
</ol>
<p>
<div class="math">$$
J_\text{test}(\theta) = \frac{1}{2m_\text{test}}\sum_{i=1}^{m_{test}}(h_\theta(x_{\text{test}}^{(i)})-y_\text{test}^{(i)})^2
$$</div>
</p>
</li>
<li>
<p>For logistic regression, we can use similar way like linear regression to calculate
test set error but there is a way due to the nature of classification task. In this
case, we also call test set error as <strong>misclassification error</strong> or <strong>(0/1 misclassification error)</strong>:</p>
</li>
</ul>
<div class="math">$$
\text{err}(h_\theta(x),y)=\left\{
                \begin{array}{ll}
                  1 \text{ if } h_\theta(x) \ge 0.5, y = 0 \text{ or if } h_\theta(x) &lt; 0.5, y = 1 \\
                  0 \text{ otherwise }
                \end{array}
              \right.
$$</div>
<p>This definition gives us a binary <span class="math">\(0\)</span> or <span class="math">\(1\)</span> error result based on a misclassification.
Then, we calculate test set error as </p>
<div class="math">$$
\text{Test error} = \frac{1}{m_\text{error}}\sum_{i=1}^{m_\text{test}}
\text{err}(h_\theta(x_\text{test}^{(i)}),y_\text{test}^{(i)})
$$</div>
<p>This gives us the proportion of the test data that was misclassified.</p>
<p>In addition to the test set error, we will define <strong>cross validation set error</strong>
as well. Instead of dividing the whole data set as training set and test set, 
we can divide it into three parts: training set, corss validation (cv) set, and
test set, with proportion of data set as <span class="math">\(60\%\)</span>, <span class="math">\(20\%\)</span>, and <span class="math">\(20\%\)</span>. Mathematically,
similar to the notation of test set, we have <span class="math">\((x_{\text{cv}}^{(1)}, y_{\text{cv}}^{(1)}), (x_{\text{cv}}^{(1)}, y_{\text{cv}}^{(1)}) \dots (x_{\text{cv}}^{(m_\text{cv})}, y_{\text{cv}}^{(m_\text{cv})})\)</span> with
<span class="math">\(m_\text{cv} = \text{no. of cv examples}\)</span>. The purpose
of dividing data set in this way will be clear in the next section. </p>
<p>Now, we summarize our metrics (training error, cross validation error, and test set error)
as follows:</p>
<div class="math">$$
\begin{eqnarray*}
J_\text{train}(\theta) &amp;=&amp; \frac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 \\
J_\text{cv}(\theta)    &amp;=&amp; \frac{1}{2m_\text{cv}}\sum_{i=1}^{m_{cv}}(h_\theta(x_{\text{cv}}^{(i)})-y_\text{cv}^{(i)})^2 \\
J_\text{test}(\theta)  &amp;=&amp; \frac{1}{2m_\text{test}}\sum_{i=1}^{m_{test}}(h_\theta(x_{\text{test}}^{(i)})-y_\text{test}^{(i)})^2
\end{eqnarray*}
$$</div>
<h2 id="overfitting-vs-underfitting">Overfitting vs. Underfitting</h2>
<p>In <a href="../../../../../posts/2017/May/05/andrew-ngs-ml-week-01-03/">week 01-03 post</a>, we mention
the term <em>overfitting</em> when we talk about regularization. Now, we explain it
in details. <strong>Overfitting</strong> happens when we have too many features, the learning
hypothesis may fit the training set very well (i.e.
<span class="math">\(J(\theta)=\frac{1}{2m} \sum_{i=1}^m(\theta^T x^{(i)}-y(i))^2 \approx 0\)</span>) but
fail to generalize to new examples (i.e. predict prices on a new set of house). 
Take a look at the following three graphs</p>
<p><img alt="overfitting example: linear regression" class="img-responsive" src="../../../../../images/overfitting.PNG"/></p>
<p>The first graph (leftmost) shows the result of fitting our training set
with a hypothesis: <span class="math">\(h_\theta(x) = \theta_0 + \theta_1 x\)</span>. You can see that
the data doesn't really lie on straight line, and so the fit is not very good.
In this case, we call the scenario <strong>underfitting</strong>, which means the model doesn't
capture the data structure well. Another term for this is <strong>high bias</strong>. One way
to think of <strong>high bias</strong> is that the algorithm has strong preconception on
what the data should be, in our case, linear. In summary, underfitting, or high bias,
is when the form of our hypothesis function <span class="math">\(h\)</span> maps poortly to the trend of the data. 
It is usually caused by a function that is too simple or uses too few features.</p>
<p>At the other extreme, shown by the rightmost graph, is <strong>overfitting</strong>, or <strong>high variance</strong>.
<strong>High variance</strong> means that the function can almost fit any function: hypothesis
<span class="math">\(h\)</span> is too general and we don't have enough data to constrain it. The overfitting or 
high variance is usually caused by a complicated function that creates a lot of unnecessary
curves and angles unrelated to the data. </p>
<p>There are a couple of ways to tackle the overfitting problem:</p>
<ol>
<li>
<p>Reduce number of features</p>
<ul>
<li>Manually select which feature to keep</li>
<li>Model selection algorithm</li>
</ul>
</li>
<li>
<p>Regularization, which can keep all the features, but reduce magnitude/values of parameters <span class="math">\(\theta_j\)</span>.
This way works well when we have a lot of features, each of which contributes a bit to predicting <span class="math">\(y\)</span>.</p>
</li>
</ol>
<h3 id="model-selection-algorithm">Model selection algorithm</h3>
<p>Once parameters <span class="math">\(\theta_0, \theta_1, \dots\)</span> were fit to some set of data (training set),
the error of the parameters as measured on that data (i.e. <span class="math">\(J_\text{train}\theta\)</span>) 
is likely to be lower than the actual generalization error. In other words, 
<span class="math">\(J_\text{train}\theta\)</span> will be a bad metric on predicting how well our hypothesis
will be generalized to new examples. So, how do we measure how well our hypothesis
will perform on new examples? In addition, how to select which model to use? Ideally,
we should pick the model that has the best performance on new examples. As you can tell,
these two questions are equivalent and are all centered around the metrics we 
use for reporting our model generalization error.</p>
<p>We can start with the following schemes to pick our model. We use
<span class="math">\(d\)</span> to denote the degree of polynomial of our model. For example,
<span class="math">\(d = 1\)</span> means <span class="math">\(h_\theta(x) = \theta_0 + \theta_1 x\)</span>; <span class="math">\(d=2\)</span> means
<span class="math">\(h_\theta(x) = \theta_0 + \theta_1 x + \theta_2 x^2\)</span>. Then, we can do:</p>
<ol>
<li>Optimize the parameters in <span class="math">\(\Theta\)</span> using the training set for each polynomial
degree <span class="math">\(d\)</span>.</li>
<li>Find the polynomial degree <span class="math">\(d\)</span> with the least error <span class="math">\(J_\text{test}(\theta)\)</span>
using the test set. We pick the model with this <span class="math">\(d\)</span> and report our
test set error <span class="math">\(J_\text{test}(\theta)\)</span> as the metric for estimate of generalization error.</li>
</ol>
<p>However, there is a problem with this scheme: we use our extra parameter <span class="math">\(d\)</span> to fit
the test set. In other words, we choose <span class="math">\(d\)</span>, then we fit with <span class="math">\(J_\text{test}(\theta)\)</span>.
Our estimate is likely optimistic and our model is likely do better on test set 
than on new examples hasn't seen before. This is similar to overfitting in training set.</p>
<p>In order to fix this problem, we introduce <strong>cross validation set</strong>. We modify
above scheme as follows:</p>
<ol>
<li>Optimize the parameters in <span class="math">\(\Theta\)</span> using the training set for each polynomial degree</li>
<li>Find the polynomial degree <span class="math">\(d\)</span> with the least error using the cross validation set</li>
<li>Estimate the generalization error using the test set with <span class="math">\(J_\text{test}(\theta^{(d)})\)</span>
(<span class="math">\(\theta^{(d)}\)</span> is the parameter <span class="math">\(\Theta\)</span> from polynomial with the lowest error)</li>
</ol>
<p>This way, our <span class="math">\(d\)</span> has not been trained using the test set. </p>
<h3 id="diagnosing-bias-vs-variance-which-is-which">Diagnosing bias vs. variance: which is which?</h3>
<p>Once we have the metrics and the understanding of cross validation set, we can
now find out whether bias or variance is the problem contributing to bad predictions. 
We have the following picture to help us understand the relationship bewtween <span class="math">\(d\)</span>
and the underfitting (high bias) or overfitting (high variance) of our hypothesis</p>
<p><img alt="bias vs. variance" class="img-responsive" src="../../../../../images/bias-variance.PNG"/></p>
<p>The training error will tend to decrease as we increase the degree <span class="math">\(d\)</span> of polynomial
because our hypothesis fitness to our training data becomes better and better. 
On the other hand, the cross validation error will tend to decrease
as we increase <span class="math">\(d\)</span> up to a point (because our model can generalize well), and
then it will increase as <span class="math">\(d\)</span> increased (because we now overfit the training data and
cannot be generalize well in cross validation set), forming a convex curve.
So now, based on the picture, we can answer the question: suppose the learning algorithm is
performing less well than you were hoping (<span class="math">\(J_\text{cv}(\theta)\)</span> or <span class="math">\(J_\text{test}(\theta)\)</span> is high).
is it a bias problem or a variance problem?</p>
<ul>
<li>
<p>High bias (underfitting): <span class="math">\(J_\text{train}(\theta)\)</span> will be high; 
<span class="math">\(J_\text{cv}(\theta) \approx J_\text{train}(\theta)\)</span></p>
</li>
<li>
<p>High variance (overfitting): <span class="math">\(J_\text{train}(\theta)\)</span> will be low;
<span class="math">\(J_\text{cv}(\theta) \gg J_\text{train}(\theta)\)</span></p>
</li>
</ul>
<h3 id="regularization-how-to-choose-lambda">Regularization: how to choose <span class="math">\(\lambda\)</span>?</h3>
<p>In the overfitting section above, we know that regularization is another way
to handle the overfitting. There is a problem with regularization method: how
do we set <span class="math">\(\lambda\)</span> appeard in the <span class="math">\(J_\theta(x)\)</span>? In general, when <span class="math">\(\lambda\)</span>
is large, we tend to underfit (i.e. high bias) the data and when <span class="math">\(\lambda\)</span> is small,
we tend to overfit (i.e. high variance). In the course, the following method
is proposed:</p>
<ol>
<li>Create a list of <span class="math">\(\lambda\)</span> (i.e. <span class="math">\(\lambda = 0, 0.01, 0.02, 0.04, \dots, 10.24\)</span> (multiple of 2))</li>
<li>Create a set of models with different degrees or any other variants</li>
<li>Iterate through <span class="math">\(\lambda\)</span>s and for each <span class="math">\(\lambda\)</span>, go through all the models
to learn some <span class="math">\(\theta\)</span>.</li>
<li>Compute the cross validation error <span class="math">\(J_{cv}(\theta)\)</span> without regularization term (i.e. <span class="math">\(\lambda = 0\)</span>)
using the learned <span class="math">\(\theta\)</span>.</li>
<li>Select the best combo that produces the lowest error on the cross validation set</li>
<li>Using the best combo <span class="math">\(\lambda\)</span> and <span class="math">\(\theta\)</span>, apply it on <span class="math">\(J_{test}(\theta)\)</span>
to see if it has a good generalization.</li>
</ol>
<p>We can also plot the Bias/Variance as a function of the regulariation parameter like below:</p>
<p><img alt="regualarization parameter" class="img-responsive" src="../../../../../images/regularization-bias-vairance.png"/></p>
<h3 id="learning-curves">Learning curves</h3>
<p>Learning curve is a tool to help us identfy whether we are facing underfitting (i.e. high bias) 
or overfitting (i.e. high variance) problem and at the same time, gives us a way to answer the question:
Will getting more training data help us improve our learning algorithm performance? </p>
<p>The following picture shows what learning curves look like for a linear regression (i.e. <span class="math">\(h_\theta(x) = \theta_0 + \theta_1 x + \theta_2 x^2\)</span>):</p>
<p><img alt="learning curves general" class="img-responsive" src="../../../../../images/learning-curves-general.png"/></p>
<p>The x-axis of the learning curves is the training set size <span class="math">\(m\)</span> and the y-axis is the error. When <span class="math">\(m\)</span> is small, any hypothesis
can fit the training data perfectly, and thus our <span class="math">\(J_{train}(\theta)\)</span> is small. However, as <span class="math">\(m\)</span> increases, our hypothesis cannot
fit all the data, and thus <span class="math">\(J_{train}(\theta)\)</span> increases. On the other hand, when <span class="math">\(m\)</span> is small, our hypothesis cannot generalize
much and thus <span class="math">\(J_{cv}(\theta)\)</span> tends to be high. However, as <span class="math">\(m\)</span> increases, the more data we have, the better hypothesis we can
get and thus our hypothesis can generalize well to new examples and <span class="math">\(J_{cv}(\theta)\)</span> decreases. </p>
<h4 id="experience-underfitting">Experience underfitting</h4>
<p>Now, let's see what learning curves look like when we face underfitting (i.e. high bias) problem. For example, we try to fit our
data with hypothesis <span class="math">\(h_\theta(x) = \theta_0 + \theta_1x\)</span>. Now, when <span class="math">\(m\)</span> is small, our <span class="math">\(J_{train}(\theta)\)</span> will be small, and it
will increase as <span class="math">\(m\)</span> increases. After certain point, our <span class="math">\(J_{train}(\theta)\)</span> will flat out because our hypothesis is a straight line
and more data won't help much. On the other hand, <span class="math">\(J_{cv}(\theta)\)</span> will be high when <span class="math">\(m\)</span> small and will decrease as <span class="math">\(m\)</span> increases. 
Similar to <span class="math">\(J_{train}(\theta)\)</span>, <span class="math">\(J_{cv}(\theta)\)</span> will quickly flat out because number of hypothesis parameter is so small and it 
won't generalize well as data increases. In other words, when we have high bias, the performance of <span class="math">\(J_{cv}(\theta)\)</span> and <span class="math">\(J_{train}(\theta)\)</span>
will look a lot similar. Thus, our learning curve looks something like this:</p>
<p><img alt="high bias learning curves" class="img-responsive" src="../../../../../images/high-bias-learning-curves.png"/></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul>
<li><span class="math">\(m\)</span> is small: causes <span class="math">\(J_{train}(\theta)\)</span> to be low and <span class="math">\(J_{cv}(\theta)\)</span> to be high.</li>
<li><span class="math">\(m\)</span> is large: causes both <span class="math">\(J_{train}(\theta)\)</span> and <span class="math">\(J_{cv}(\theta)\)</span> to be high with <span class="math">\(J_{train}(\theta) \approx J_{cv}(\theta)\)</span></li>
</ul>
</div>
<p>From above graph we can get, if a learning algorithm is suffering from high bias, getting more training data will <strong>not</strong>
(by itself) help much.</p>
<h4 id="experience-overfitting">Experience overfitting</h4>
<p>For overfitting (i.e. high variance) problem, let's consider a hypothesis: <span class="math">\(h_\theta(x) = \theta_0 + \theta_1x + \dots + \theta_{100}x^{100}\)</span> 
(and small <span class="math">\(\lambda\)</span>). When <span class="math">\(m\)</span> is small, <span class="math">\(J_{train}(\theta)\)</span> is small because we fit very small data size with very high degree polynomial.
As <span class="math">\(m\)</span> increases, <span class="math">\(J_{train}(\theta)\)</span> increases but not so much because with high polynomial degree, even we cannot fit the data perfectly,
our hypothesis is still pretty good with large data size. For <span class="math">\(J_{cv}(\theta)\)</span>, we have overfitting problem no matter the size of <span class="math">\(m\)</span>. 
Even <span class="math">\(m\)</span> increases may help our hypothesis generalize better, our hypothesis can still do poorly for new examples. So, the learning curves
for high variance problem looks like below:</p>
<p><img alt="high variance learning curves" class="img-responsive" src="../../../../../images/high-variance-learning-curves.png"/></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul>
<li><span class="math">\(m\)</span> is small: <span class="math">\(J_{train}(\theta)\)</span> will be low and <span class="math">\(J_{cv}(\theta)\)</span> will be high.</li>
<li><span class="math">\(m\)</span> is large: <span class="math">\(J_{train}(\theta)\)</span> increases with training set size and <span class="math">\(J_{cv}(\theta)\)</span> continues to decrease without
levelling off. Also <span class="math">\(J_{train}(\theta) &lt; J_{cv}(\theta)\)</span> but the difference between them remains significant.</li>
</ul>
</div>
<p>From above graph, if we keep getting more data, <span class="math">\(J_{cv}(\theta)\)</span> will keep getting down and this indicates that if a learning algorithm
is suffering from high variance, getting more training data is likely to help.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the learning curves, <span class="math">\(J_{cv}(\theta)\)</span> can be substituted with <span class="math">\(J_{test}(\theta)\)</span> and the shape still holds. All in all, we really
care about the size of <span class="math">\(J_{cv}(\theta)\)</span> (or <span class="math">\(J_{test}(\theta)\)</span>).</p>
</div>
<h2 id="what-to-try-next">What to try next?</h2>
<p>Now, we can answer the question appeard in the perface section: suppose we have implemented regularized linear regression to predict
housing prices. However, when we test our hypothesis in a new set of houses, we find that it makes unacceptablely large errors in
its prediction. What we should try next?</p>
<table class="table-hover  table-striped table">
<thead>
<tr>
<th>Method</th>
<th>When it works?</th>
</tr>
</thead>
<tbody>
<tr>
<td>Get more training examples</td>
<td>high variance</td>
</tr>
<tr>
<td>Try smaller sets of features</td>
<td>high variance</td>
</tr>
<tr>
<td>Try getting additional features</td>
<td>high variance</td>
</tr>
<tr>
<td>Try adding polynomial features (i.e. <span class="math">\(x_1^{2}\)</span>, <span class="math">\(x_2^{2}\)</span>, <span class="math">\(x_1x_2\)</span>)</td>
<td>high bias</td>
</tr>
<tr>
<td>Try decreasing <span class="math">\(\lambda\)</span></td>
<td>high bias</td>
</tr>
<tr>
<td>Try increasing <span class="math">\(\lambda\)</span></td>
<td>high variance</td>
</tr>
</tbody>
</table>
<p>This leads to model complexity effects:</p>
<ul>
<li>Lower-order polynomial (low model complexity) have high bias and low variance. The model fits poorly consistently.</li>
<li>Higher-order polynomial (high model complexity) fits training data extremely well but test data poorly. Low bias but high variance on
training data. </li>
</ul>
<p>Ideally, we should choose a model somewhere in between.</p>
<h2 id="neural-network-and-overfitting">Neural Network and overfitting</h2>
<p>Underfitting and overfitting also exist in the <a href="../../../../../posts/2017/May/23/andrew-ngs-ml-week-04-05/">neural network</a>. When
we use "small" neural network (i.e. less hidden layers, less hidden units), we have fewer parameters and more prone
to underfitting. In the contrast, when we use "large" neural network, it's computationally more expensive and 
more parameters means more prone to overfitting. In this case, we use <span class="math">\(\lambda\)</span> to address the issue.</p>
<p>We also face the similar model selection problem like we have when working with linear regression. In the neural network
setting, that means deciding number of hidden layers we need to use in the network. Prof. Ng talks about the following
method to solve the problem:</p>
<ol>
<li>We create a list of number of hidden layers</li>
<li>For each number of hidden layers, we optimize the parameters in <span class="math">\(\Theta\)</span> using the training set</li>
<li>Find the number of hidden layers with the least error using the cross validation set</li>
</ol>
<p>Usually, using a single hidden layer is a good starting default. We can then train our neural network
on a number of hidden layers using our cross validation set. We can then choose the one that performs the
best.</p>
<h2 id="error-analysis">Error analysis</h2>
<p>Error analysis means manually examine the examples (i.e. cross validation set) that your algorithm 
made errors on and see if you spot any systematic trend in what type of examples it is making errors on.
Then, we need to try some method to see if it helps. The key point during the whole analysis is that
we need to come up some numeric evaluation, which gives a single raw number, to determine how system works.</p>
<p>This leads to a recommended approach for handling machine learning problem:</p>
<ul>
<li>Start with a simple algorithm that you can implement quickly. Implement it and test it on your cross-validation set.</li>
<li>Plot learning curves to decide if more data, more features, etc, are likely to help</li>
<li>Perform error analysis</li>
</ul>
<h2 id="ceiling-analysis">Ceiling analysis</h2>
<p>Ceiling analysis is helpful when we work on a machine learning system, which contains many components. Then ceiling 
analysis tries to address the question: what part of the pipeline we need to focus on to improve next? This is done
by estimating the errors due to each component. Suppose we have a image recognition system with four components:</p>
<div class="math">$$
\text{images} \Rightarrow \text{Text detection} \Rightarrow \text{character segmentation} \Rightarrow \text{character recognition}
$$</div>
<p>Then, we try to decide which part of the pipeline we should spend the most time trying to improve. </p>
<table class="table-hover  table-striped table">
<thead>
<tr>
<th>component</th>
<th>Accuracy of overall system</th>
</tr>
</thead>
<tbody>
<tr>
<td>Overall system</td>
<td>72%</td>
</tr>
<tr>
<td>Text detection</td>
<td>89%</td>
</tr>
<tr>
<td>Character segmentation</td>
<td>90%</td>
</tr>
<tr>
<td>Character recognition</td>
<td>100%</td>
</tr>
</tbody>
</table>
<p>Before we improve some component of the system, we have a overall system accuracy of <span class="math">\(72\%\)</span>. Now, let's
take text detection as an example. Let's manually label where the text is and this will give <span class="math">\(100\%\)</span> accuracy
of text detection. Then, we run the rest modules, and get an overall system accuracy, which in our case is <span class="math">\(89\%\)</span>.
We perform the similar steps for each component. Then, we calculate the gain of system accuracy. For example,
we will get <span class="math">\(17\%\)</span> gain by working on text detection (i.e. <span class="math">\(89\% - 72\%\)</span>) and <span class="math">\(1\%\)</span> by working on character segmentation,
and <span class="math">\(10\%\)</span> by working on character recognition. As you can see, text detection will give us the largest gain, and thus
we should work on this componet next.  </p>
<h2 id="other-issues">Other issues</h2>
<p>Machine learning is interesting because there probably doesn't exist a unified way to handle different kind of data.
This means for some special data, we may need to have some special approach to handle them.</p>
<h3 id="error-metrics-for-skewed-classes-precision-recall">Error metrics for skewed classes: Precision &amp; Recall</h3>
<h4 id="skew-classes">Skew classes</h4>
<p>Consider the following example: we want to do a cancer classification using logistic regression with 
<span class="math">\(y=1\)</span> indicating cancer and <span class="math">\(y=0\)</span> otherwise. After training on the training data, we find that
we got <span class="math">\(1\%\)</span> error on test set (i.e. <span class="math">\(99\%\)</span> correct diagnoses). Can we say that our learning algorithm 
is performing well? The answer is depends. We further examine the training data and find out that
only <span class="math">\(0.5\%\)</span> of patients have cancer. This causes problem to our training task because we can directly
set <span class="math">\(y=0\)</span> for every training data and we will get only <span class="math">\(0.5\%\)</span> error, which is less than <span class="math">\(1%\)</span> in our previous 
case. However, this error is useless. This is a typical scenario of <strong>skew classes</strong>, where the number of
possitive examples <span class="math">\(\ll\)</span> the number of negative examples. </p>
<h4 id="precision-recall">Precision &amp; Recall</h4>
<p>Rather than using classification error as a measurement to our learning algorithm performance, we 
use precision and recall instead when we deal with skewed class. </p>
<ul>
<li>Let <span class="math">\(y = 1\)</span> in presence of <em>rare class</em> that we want to detect</li>
<li><span class="math">\(\text{Precision} = \frac{\text{True positives}}{\text{# predicted positives}} = \frac{\text{True positive}}{\text{True positive}+\text{False positive}}\)</span>
(i.e. of all patients where we predicted <span class="math">\(y = 1\)</span>, what fraction actually have cancer?)</li>
<li><span class="math">\(\text{Recall} = \frac{\text{True positives}}{\text{# actual positives}} = \frac{\text{True positives}}{\text{True positives}+\text{False negatives}}\)</span>
(i.e. of all patients that actually have cancer, what fraction did we correctly detect as having cancer?)</li>
</ul>
<p><img alt="precision and recall" class="img-responsive" src="../../../../../images/precision-recall.png"/></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If we apply this concept to the previous scenario when we set <span class="math">\(y=0\)</span> for each training data, then the recall will be <span class="math">\(0\)</span>.</p>
</div>
<p>There is a tradeoff between precision and recall. Suppose we have a logistic regression: <span class="math">\(0 \ge h_\theta(x) \le 1\)</span> and we want to predict
<span class="math">\(1\)</span> if <span class="math">\(h_\theta(x) \ge \text{threshold}\)</span> and <span class="math">\(0\)</span> if <span class="math">\(h_\theta(x) \le \text{threshold}\)</span>. Then, how do we determine that threhold value?</p>
<ul>
<li>Suppose we want to predict <span class="math">\(y=1\)</span> (i.e. cancer) only if very confident: Higher precision, lower recall (i.e. threhold = <span class="math">\(0.7\)</span>)</li>
<li>Suppose we want to avoid missing too many cases of cancer (i.e. avoid false negatives): Higher recall, lower precision (i.e. threshold = <span class="math">\(0.3\)</span>)</li>
</ul>
<p>As you can see, we cannot maintain high precision and high recall at the same time. This kind of tradeoff is dicpted in the picture below: </p>
<p><img alt="precision and recall tradeoff" class="img-responsive" src="../../../../../images/precision-recall-tradeoff.png"/></p>
<h4 id="f_1-score"><span class="math">\(F_1\)</span> score</h4>
<p>Now, we may ask if there is a way to choose our threshold value automatically. That's where <span class="math">\(F_1\)</span> score comes from. If we use <span class="math">\(P\)</span> to denote
the precision and <span class="math">\(R\)</span> to denote recall, then our <span class="math">\(F_1\)</span> score is defined as follows</p>
<div class="math">$$
F_1 \text{ score} = 2 \frac{PR}{P+R}
$$</div>
<ul>
<li>If <span class="math">\(P = 0\)</span> or <span class="math">\(R = 0\)</span>, then <span class="math">\(F_1 \text{ score} = 0\)</span></li>
<li>If <span class="math">\(P = 1\)</span> and <span class="math">\(R = 1\)</span>, then <span class="math">\(F_1 \text{ score} = 1\)</span></li>
</ul>
<p>Then, we can pick the threshold value by measuring <span class="math">\(P\)</span> and <span class="math">\(R\)</span> on the cross validation set and choose the value of threshold which maximizes our 
<span class="math">\(F_1 \text{ score}\)</span>.</p>
<h3 id="data-for-machine-learning">Data for machine learning</h3>
<p>When we should use a very large training set? Two things need to consider before we do that:</p>
<ul>
<li>Assume feature <span class="math">\(x \in R^{n+1}\)</span> has sufficient information to predict <span class="math">\(y\)</span> accurately. This can be tested by answering the question: given the input
<span class="math">\(x\)</span>, can a human expert confidently predict <span class="math">\(y\)</span>?</li>
<li>Use a learning algorithm with many parameters (i.e. logistic regression or linear regression with many features; neural network with many hidden units),
which can give us a low bias algorithm.</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </article>

<!--        <footer>
            <p>This entry is posted in <a href="../../../../../category/machine-learning.html">Machine Learning</a>.</p>
        </footer>-->

<div class="comments">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        var disqus_shortname = 'zhu45-org';
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

    </div>


<script>
   function topFunction() {
       document.body.scrollTop = 0;
       document.documentElement.scrollTop = 0;
   }
</script>

<footer class="blog-footer">
    <div id="copyright">
      Copyright (c) 2015-2019 <a href="../../../../../about-me.html">Zeyuan Hu</a>
    </div>
    <div id="archive">
      <a href="javascript:topFunction();">Back to top</a>
    </div>
</footer>
            </div>
<script>
    var _gaq=[['_setAccount','UA-37565522-2'],['_trackPageview']];
    (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
    g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
    s.parentNode.insertBefore(g,s)}(document,'script'));
</script>
    </body>
</html>